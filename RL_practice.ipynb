{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/mukilan/intro_to_gym/reports/A-Gentle-Introduction-to-OpenAI-Gym--VmlldzozMjg5MTA3#:~:text=OpenAI%20Gym%20is%20a%20Pythonic,toolkit%20for%20training%20RL%20algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole is the Hello World for reinforcement learning. It is a simple environment in which there is a cart with a pole on top of the cart. The cart has some initial velocity, it should learn to adjust its velocity and actions to balance the pole and avoid falling. \n",
    "\n",
    "Action Space: Discrete (2); Left or Right\n",
    "\n",
    "Observation Space: Array with the shape (4). It contains 4 signals:\n",
    "\n",
    "Cart Position\n",
    "\n",
    "Cart Velocity\n",
    "\n",
    "Pole Angle\n",
    "\n",
    "Pole Angular Velocity\n",
    "\n",
    "\n",
    "Reward Function: +1 for every time step the pole is standing. \n",
    "\n",
    "Termination: When the episode reward exceeds 475, the episode is terminated. \n",
    "\n",
    "We'll set the configuration dictionary with the environment name, type of policy, and total time steps. We'll pass this config dictionary to our run which is initialized with wandb.init(). It will automatically log all the necessary metrics. We'll also pass in monitor_gym = True to auto-upload training videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "env.action_space\n",
    "env.observation_space.shape\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01463811  0.1482173   0.00112455 -0.33089066]\n",
      "1.0\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "#len(env.step(env.action_space.sample()))\n",
    "print(env.step(1)[0])\n",
    "print(env.step(1)[1])\n",
    "print(env.step(1)[2])\n",
    "print(env.step(1)[3])\n",
    "print(env.step(1)[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m         env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      5\u001b[0m     state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[0;32m----> 6\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m      7\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:298\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "done = True\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FlattenObservation\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'CartPole/train/'\n",
    "LOG_DIR = 'CartPole/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=50, save_path=CHECKPOINT_DIR)\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to CartPole/logs/PPO_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 512      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 23.7          |\n",
      "|    ep_rew_mean          | 23.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 47            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3993274e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00239       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 52.3          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -8.41e-06     |\n",
      "|    value_loss           | 105           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 23.3          |\n",
      "|    ep_rew_mean          | 23.3          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 47            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 1536          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3329707e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00179       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 42.9          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -7.89e-05     |\n",
      "|    value_loss           | 86.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.7          |\n",
      "|    ep_rew_mean          | 22.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1676457e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00018       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 43.8          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -3.03e-05     |\n",
      "|    value_loss           | 98.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 21.7          |\n",
      "|    ep_rew_mean          | 21.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4400651e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00136       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 48.1          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -5.58e-05     |\n",
      "|    value_loss           | 86.3          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 20.4          |\n",
      "|    ep_rew_mean          | 20.4          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 65            |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8370414e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00074      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 36.7          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -7e-05        |\n",
      "|    value_loss           | 73.8          |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 20.4           |\n",
      "|    ep_rew_mean          | 20.4           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 46             |\n",
      "|    iterations           | 7              |\n",
      "|    time_elapsed         | 76             |\n",
      "|    total_timesteps      | 3584           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.03143975e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.693         |\n",
      "|    explained_variance   | -0.000704      |\n",
      "|    learning_rate        | 1e-06          |\n",
      "|    loss                 | 33.9           |\n",
      "|    n_updates            | 60             |\n",
      "|    policy_gradient_loss | -2.17e-05      |\n",
      "|    value_loss           | 68.7           |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 20.3          |\n",
      "|    ep_rew_mean          | 20.3          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 87            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1455572e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00227      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 41.2          |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -1.42e-05     |\n",
      "|    value_loss           | 83.1          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 20.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 4608         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.934838e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | -0.000293    |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 41.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -1.03e-05    |\n",
      "|    value_loss           | 81.2         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.2          |\n",
      "|    ep_rew_mean          | 22.2          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 109           |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0209624e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00106       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 38.6          |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    value_loss           | 74.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.8          |\n",
      "|    ep_rew_mean          | 22.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 120           |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4714973e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00451      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 62.7          |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -8.19e-05     |\n",
      "|    value_loss           | 111           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 24.8          |\n",
      "|    ep_rew_mean          | 24.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 131           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7893035e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00238      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 58.3          |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -6.94e-05     |\n",
      "|    value_loss           | 121           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 24            |\n",
      "|    ep_rew_mean          | 24            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 142           |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9697625e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00453       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 47.2          |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -1.18e-05     |\n",
      "|    value_loss           | 97.9          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 24.1         |\n",
      "|    ep_rew_mean          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.018672e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.00272      |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 35.3         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -1.42e-05    |\n",
      "|    value_loss           | 66.5         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 23.8          |\n",
      "|    ep_rew_mean          | 23.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 164           |\n",
      "|    total_timesteps      | 7680          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7171336e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00185       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 43.5          |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -7.25e-05     |\n",
      "|    value_loss           | 91.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 21.8          |\n",
      "|    ep_rew_mean          | 21.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 175           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4947727e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00204      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 44.9          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -4.16e-05     |\n",
      "|    value_loss           | 87.5          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 22.2         |\n",
      "|    ep_rew_mean          | 22.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 186          |\n",
      "|    total_timesteps      | 8704         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.967419e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.00217      |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 40.1         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -5.1e-05     |\n",
      "|    value_loss           | 78.1         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.5          |\n",
      "|    ep_rew_mean          | 22.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 196           |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1455268e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00371      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 49            |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -3.05e-05     |\n",
      "|    value_loss           | 87.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.8          |\n",
      "|    ep_rew_mean          | 22.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 207           |\n",
      "|    total_timesteps      | 9728          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7206337e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.00361       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 37.5          |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000111     |\n",
      "|    value_loss           | 79.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.2          |\n",
      "|    ep_rew_mean          | 22.2          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 218           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9546133e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | 0.000715      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 55            |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -7.39e-05     |\n",
      "|    value_loss           | 103           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x75fa6c5b3390>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('CartPole', 'Saved_Models')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_policy\n\u001b[1;32m      2\u001b[0m evaluate_policy(model, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m run\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo.hurtado/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DummyVecEnv' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Start the game\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_env()\n\u001b[0;32m----> 6\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(vec_env)\n\u001b[1;32m      7\u001b[0m obs \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/env_util.py:125\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[0;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vec_env_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Default: use a DummyVecEnv\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     vec_env_cls \u001b[38;5;241m=\u001b[39m DummyVecEnv\n\u001b[0;32m--> 125\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m vec_env_cls([make_env(i \u001b[38;5;241m+\u001b[39m start_index) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs)], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvec_env_kwargs)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Prepare the seeds for the first reset\u001b[39;00m\n\u001b[1;32m    127\u001b[0m vec_env\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(fn()) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(fn()) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/env_util.py:98\u001b[0m, in \u001b[0;36mmake_vec_env.<locals>.make_env.<locals>._init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_id(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     env \u001b[38;5;241m=\u001b[39m _patch_env(env)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DummyVecEnv' object is not callable"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "model = PPO.load('CartPole/train/best_model_4200.zip', env=env)\n",
    "#Start the game\n",
    "vec_env = model.get_env()\n",
    "vec_env = make_vec_env(vec_env)\n",
    "obs = vec_env.reset()\n",
    "\n",
    "#print(model.predict(obs))\n",
    "#Loop through the game\n",
    "#while True:\n",
    "#    action, _state = model.predict(obs)\n",
    "#    state, reward, done, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('CartPole/train/best_model_5000.zip')\n",
    "state = env.reset()\n",
    "print(model.predict(state[0])[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
