{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/mukilan/intro_to_gym/reports/A-Gentle-Introduction-to-OpenAI-Gym--VmlldzozMjg5MTA3#:~:text=OpenAI%20Gym%20is%20a%20Pythonic,toolkit%20for%20training%20RL%20algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole is the Hello World for reinforcement learning. It is a simple environment in which there is a cart with a pole on top of the cart. The cart has some initial velocity, it should learn to adjust its velocity and actions to balance the pole and avoid falling. \n",
    "\n",
    "Action Space: Discrete (2); Left or Right\n",
    "\n",
    "Observation Space: Array with the shape (4). It contains 4 signals:\n",
    "\n",
    "Cart Position\n",
    "\n",
    "Cart Velocity\n",
    "\n",
    "Pole Angle\n",
    "\n",
    "Pole Angular Velocity\n",
    "\n",
    "\n",
    "Reward Function: +1 for every time step the pole is standing. \n",
    "\n",
    "Termination: When the episode reward exceeds 475, the episode is terminated. \n",
    "\n",
    "We'll set the configuration dictionary with the environment name, type of policy, and total time steps. We'll pass this config dictionary to our run which is initialized with wandb.init(). It will automatically log all the necessary metrics. We'll also pass in monitor_gym = True to auto-upload training videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "env.action_space\n",
    "env.observation_space.shape\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "#len(env.step(env.action_space.sample()))\n",
    "print(env.step(1)[0])\n",
    "print(env.step(1)[1])\n",
    "print(env.step(1)[2])\n",
    "print(env.step(1)[3])\n",
    "print(env.step(1)[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FlattenObservation\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'CartPole/train/'\n",
    "LOG_DIR = 'CartPole/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=50, save_path=CHECKPOINT_DIR)\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('CartPole', 'Saved_Models')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "model = PPO.load('CartPole/train/best_model_4200.zip', env=env)\n",
    "#Start the game\n",
    "vec_env = model.get_env()\n",
    "vec_env = make_vec_env(vec_env)\n",
    "obs = vec_env.reset()\n",
    "\n",
    "#print(model.predict(obs))\n",
    "#Loop through the game\n",
    "#while True:\n",
    "#    action, _state = model.predict(obs)\n",
    "#    state, reward, done, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('CartPole/train/best_model_5000.zip')\n",
    "state = env.reset()\n",
    "print(model.predict(state[0])[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
